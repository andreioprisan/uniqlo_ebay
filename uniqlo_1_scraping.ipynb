{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pages = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 search page took 504.74946308135986 seconds to fetch.\n",
      "1 search page took 463.47371315956116 seconds to fetch.\n",
      "1 search page took 508.8712911605835 seconds to fetch.\n",
      "fetching process is complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "search_urls = ['https://www.ebay.com/sch/i.html?_from=R40&_nkw=uniqlo+shirt&_sacat=0&LH_TitleDesc=0&_osacat=0&rt=nc&LH_Sold=1&LH_Complete=1&_ipg=192&_pgn=%s' \\\n",
    "         %(str(i + 1)) for i in range(num_pages)]\n",
    "\n",
    "from time import time\n",
    "\n",
    "'''\n",
    "    Fetch and clean transaction date, location, and seller score data \n",
    "    for an individual sold item page that is not still a current listing\n",
    "'''\n",
    "def get_item_info_all_sold(soup):\n",
    "    trans_date = soup.find('span', {'id':'bb_tlft'}).get_text()\n",
    "    location = soup.find_all('tr', 'vi-ht20')[-2].find_all('div', 'u-flL')[1].get_text()\n",
    "    seller_score = soup.find_all('span', 'mbg-l')[0].get_text()\n",
    "    return [clean_time(trans_date), location, clean_seller_score(seller_score)]\n",
    "\n",
    "'''\n",
    "    Fetch and clean transaction date, location, and seller score data \n",
    "    for an individual sold item page that is still a current listing\n",
    "'''\n",
    "def get_item_info_still_left(soup):\n",
    "    location = soup.find('span', attrs = {'itemprop':'availableAtOrFrom'}).get_text()\n",
    "    seller_score = soup.find('span', 'mbg-l').find('a').get_text()\n",
    "    return [None, location, seller_score]\n",
    "\n",
    "'''\n",
    "    Fetch and clean transaction date, location, and seller score data \n",
    "    for an individual sold item page that has a \"hybrid\" banner transition layout\n",
    "'''\n",
    "def get_item_info_hybrid(soup):\n",
    "    seller_score = soup.find_all('span','app-sellerpresence__feedbackscore')[0].get_text()\n",
    "    location = soup.find_all('span','cc-textblock--block')[0].get_text()\n",
    "    return [None, location, seller_score]\n",
    "\n",
    "'''\n",
    "    Cleans a time string\n",
    "'''\n",
    "def clean_time(time):\n",
    "    if time != None:\n",
    "        return time[5:].replace('\\n\\n', ' ')[:-1] \n",
    "    return None\n",
    "\n",
    "'''\n",
    "    Cleans a seller score\n",
    "'''\n",
    "def clean_seller_score(score):\n",
    "    return score.replace(' ', '')[3:-2]\n",
    "\n",
    "'''\n",
    "    Fetches sold item's individual page info to sold listings dataframe\n",
    "'''\n",
    "def get_single_page_info(listings_df):\n",
    "    date_list = []\n",
    "    loc_list = []\n",
    "    score_list = []\n",
    "    for link in listings_df['link']:\n",
    "        item_page = requests.get(link)\n",
    "        soup = BeautifulSoup(item_page.content, 'html.parser')\n",
    "        \n",
    "        #sold page \n",
    "        if soup.find('span', {'id':'bb_tlft'}) != None:\n",
    "            item_info = get_item_info_all_sold(soup)\n",
    "        \n",
    "        #still active listing page (multiple quantities)\n",
    "        elif soup.find('span', attrs = {'itemprop':'availableAtOrFrom'}) != None:\n",
    "            item_info = get_item_info_still_left(soup)\n",
    "        \n",
    "        #hybrid transition page (weird notice banner without showing)\n",
    "        elif soup.find('span','app-sellerpresence__feedbackscore') != None:\n",
    "            item_info = get_item_info_hybrid(soup)\n",
    "        \n",
    "        #exhausted all possible individual pages, flag as bad\n",
    "        elif soup.find('a', 'nodestar-item-card-details__view-link') == None:\n",
    "            item_info = ['BAD', 'BAD', 'BAD']\n",
    "        \n",
    "        #has banner that has another link accessible\n",
    "        else:\n",
    "            link_2 = soup.find('a', 'nodestar-item-card-details__view-link')['href']\n",
    "            item_page = requests.get(link_2)\n",
    "            soup = BeautifulSoup(item_page.content, 'html.parser')\n",
    "            item_info = get_item_info_still_left(soup)\n",
    "        date_list.append(item_info[0])\n",
    "        loc_list.append(item_info[1])\n",
    "        score_list.append(item_info[2])\n",
    "        \n",
    "    return pd.DataFrame({'link':listings_df['link'], 'date':date_list, \\\n",
    "                                'location':loc_list, 'score':score_list})\n",
    "\n",
    "'''\n",
    "    Gets a dataframe of which each row is an item from the sold items page with\n",
    "    info from both the overall and individual page incorporated\n",
    "'''\n",
    "def get_listings_df(search_url):\n",
    "    t0 = time()\n",
    "    search_page = requests.get(search_url)\n",
    "    soup = BeautifulSoup(search_page.content, 'html.parser')\n",
    "    #each listings page has weird first header content\n",
    "    listings = soup.find_all('li', attrs={'class': 's-item'})[1:]\n",
    "    \n",
    "    title_list = []\n",
    "    sec_info_list = []\n",
    "    link_list = []\n",
    "    sold_at_lower_p_list = []\n",
    "    price_list = []\n",
    "    shipping_list = []\n",
    "    purchase_type_list = []\n",
    "    num_bid_list = []\n",
    "    for item in listings:\n",
    "        title = item.find('h3', 's-item__title').get_text()\n",
    "        sec_info = item.find('span', 'SECONDARY_INFO').get_text()\n",
    "        link = item.find('a')['href']\n",
    "        sold_at_lower_price = True if item.find('span', 'STRIKETHROUGH POSITIVE') != None else False\n",
    "        price = item.find('span', 'POSITIVE').get_text()\n",
    "        shipping = item.find('span', 's-item__shipping s-item__logisticsCost').get_text()\n",
    "        purchase_info = item.find('span', 's-item__purchase-options s-item__purchaseOptions')\n",
    "        purchase_type = purchase_info.get_text() if purchase_info != None else None\n",
    "        bid_info = item.find('span', 's-item__bids s-item__bidCount')\n",
    "        num_bids = bid_info.get_text() if bid_info != None else 0\n",
    "\n",
    "        title_list.append(title)\n",
    "        sec_info_list.append(sec_info)\n",
    "        link_list.append(link)\n",
    "        sold_at_lower_p_list.append(sold_at_lower_price)\n",
    "        price_list.append(price)\n",
    "        shipping_list.append(shipping)\n",
    "        purchase_type_list.append(purchase_type)\n",
    "        num_bid_list.append(num_bids)\n",
    "    \n",
    "    listings_df = pd.DataFrame({'title':title_list, 'secondary_info':sec_info_list, 'link':link_list, \\\n",
    "                         'sold_at_lower_price':sold_at_lower_p_list, 'price':price_list, \\\n",
    "                         'shipping':shipping_list, 'purchase_type':purchase_type_list, 'num_bids':num_bid_list})\n",
    "    final_listings_df = get_single_page_info(listings_df).merge(listings_df, on = 'link')\n",
    "    \n",
    "    t1 = time()\n",
    "    print('1 search page took %s seconds to fetch.' %(t1 - t0))\n",
    "    \n",
    "    return final_listings_df\n",
    "\n",
    "'''\n",
    "    Gets overall sold listings dataframe out of a sold listings url list\n",
    "'''\n",
    "def get_final_item_df(search_urls):\n",
    "    return pd.concat([get_listings_df(search_url) for search_url in search_urls])\n",
    "\n",
    "listings_df = get_final_item_df(search_urls)\n",
    "\n",
    "listings_df.to_csv('scraped_uniqlo_data.csv')\n",
    "\n",
    "print('fetching process is complete')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
